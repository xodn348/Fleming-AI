@inproceedings{dosovitskiy2021vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021},
  organization={Google Research, Brain Team},
  url={https://arxiv.org/abs/2010.11929}
}

@inproceedings{chen2021mocov3,
  title={An Empirical Study of Training Self-Supervised Vision Transformers},
  author={Chen, Xinlei and Xie, Saining and He, Kaiming},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2021},
  organization={Facebook AI Research (FAIR)},
  url={https://arxiv.org/abs/2104.02057}
}

@inproceedings{touvron2021deit,
  title={Training Data-Efficient Image Transformers \& Distillation through Attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={10347--10357},
  year={2021},
  organization={PMLR},
  url={https://arxiv.org/abs/2012.12877}
}

@article{convnet_vs_transformer2023,
  title={ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy},
  author={Multiple Authors},
  journal={arXiv preprint arXiv:2311.09215},
  year={2023},
  url={https://arxiv.org/abs/2311.09215}
}

@article{battle_backbones2023,
  title={Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks},
  author={Multiple Authors},
  journal={arXiv preprint arXiv:2310.19909},
  year={2023},
  url={https://arxiv.org/abs/2310.19909}
}

@article{smith2023convnets,
  title={ConvNets Match Vision Transformers at Scale},
  author={Smith, Samuel L. and Brock, Andrew and Berrada, Leonard and De, Soham},
  journal={arXiv preprint arXiv:2310.16764},
  year={2023},
  organization={DeepMind},
  url={https://arxiv.org/abs/2310.16764}
}

@article{raghu2021vit_see,
  title={Do Vision Transformers See Like Convolutional Neural Networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2108.08810},
  year={2021},
  url={https://arxiv.org/abs/2108.08810}
}

@article{touvron2022three_things,
  title={Three Things Everyone Should Know about Vision Transformers},
  author={Touvron, Hugo and Cord, Matthieu and El-Nouby, Alaaeldin and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2203.09795},
  year={2022},
  url={https://arxiv.org/abs/2203.09795}
}

@article{beyer2022better_vit,
  title={Better Plain ViT Baselines for ImageNet-1k},
  author={Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
  journal={arXiv preprint arXiv:2205.01580},
  year={2022},
  organization={Google Research},
  url={https://arxiv.org/abs/2205.01580}
}

@article{shekhar2023objectives,
  title={Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations},
  author={Shekhar, Shashank and Bordes, Florian and Vincent, Pascal and Morcos, Ari},
  journal={arXiv preprint arXiv:2304.13089},
  year={2023},
  url={https://arxiv.org/abs/2304.13089}
}

@article{park2023what_do_ssl,
  title={What Do Self-Supervised Vision Transformers Learn?},
  author={Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo},
  journal={arXiv preprint arXiv:2305.00729},
  year={2023},
  url={https://arxiv.org/abs/2305.00729}
}

@article{he2020moco,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  url={https://arxiv.org/abs/1911.05722}
}

@article{chen2020simclr,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2020},
  url={https://arxiv.org/abs/2002.05709}
}

@article{caron2021dino,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:2104.14294},
  year={2021},
  url={https://arxiv.org/abs/2104.14294}
}

@article{he2022mae,
  title={Masked Autoencoders Are Scalable Vision Learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  url={https://arxiv.org/abs/2111.06377}
}
